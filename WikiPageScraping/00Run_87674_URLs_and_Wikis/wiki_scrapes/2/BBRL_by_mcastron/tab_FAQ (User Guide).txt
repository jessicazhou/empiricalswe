[<div class="wiki-body gollum-markdown-content instapaper_body" id="wiki-body">
        <div class="markdown-body">
          <h1>
<a aria-hidden="true" class="anchor" href="#1-which-agents-are-implemented" id="user-content-1-which-agents-are-implemented"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>1. Which agents are implemented?</h1>
<ul>
<li>A random agent<sup><a href="#footnotes">1</a></sup>
</li>
<li>An optimal agent<sup><a href="#footnotes">2</a></sup>
</li>
<li>e-Agent<sup><a href="#footnotes">3</a></sup>
</li>
<li>e-Greedy<sup><a href="#footnotes">4</a></sup>
</li>
<li>Soft max<sup><a href="#footnotes">5</a></sup>
</li>
<li><a href="http://www.tokic.com/www/tkicm/publikationen/papers/AdaptiveEpsilonGreedyExploration.pdf" rel="nofollow">VDBE e-Greedy</a></li>
<li><a href="http://orbi.ulg.ac.be/handle/2268/127985" rel="nofollow">Formula-based agent</a></li>
<li>
<a href="http://www.cs.mcgill.ca/%7Eaguez/files/guez_nips2012.pdf" rel="nofollow">Bayes-Adaptive Monte-Carlo Planning (BAMCP)</a><sup><a href="#footnotes">6</a></sup>
</li>
<li>
<a href="http://icaps11.icaps-conference.org/proceedings/mcts/asmuth-littman.pdf" rel="nofollow">Bayesian Forward Search Sparse Sampling (BFSSS or BFS3)</a><sup><a href="#footnotes">6</a></sup>
</li>
<li>
<a href="http://www.cs.mcgill.ca/%7Epcastr/castro_342.pdf" rel="nofollow">Smarter Best Of Sampled Set (SBOSS)</a><sup><a href="#footnotes">6</a></sup>
</li>
<li>
<a href="http://ai.stanford.edu/%7Eang/papers/icml09-NearBayesianExplorationPolynomialTime-full.pdf" rel="nofollow">Bayesian Exploration Bonus (BEB)</a><sup><a href="#footnotes">6</a></sup>
</li>
<li><a href="http://orbi.ulg.ac.be/handle/2268/127985" rel="nofollow">Offline Prior-based Policy Search for Discrete Spaces (OPPS-DS)</a></li>
<li>Offline Prior-based Policy Search for Continuous Spaces (OPPS-CS)<sup><a href="#footnotes">7</a></sup>
</li>
<li>Artifical Neural Networks for Bayesian Reinforcement Learning (ANN-BRL)<sup><a href="#footnotes">8</a></sup>
</li>
</ul>
<h1>
<a aria-hidden="true" class="anchor" href="#2-where-can-i-find-new-experiments" id="user-content-2-where-can-i-find-new-experiments"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>2. Where can I find new experiments?</h1>
<p>We provided several distributions in <code>data/distributions/</code>. You can build new experiments by following <a href="https://github.com/mcastron/BBRL/wiki/Run-experiments#step-2---create-an-experiment">those instructions</a>.</p>
<h1>
<a aria-hidden="true" class="anchor" href="#3-can-you-give-a-complete-example" id="user-content-3-can-you-give-a-complete-example"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>3. Can you give a complete example?</h1>
<p>Let us use the Uniform Generalised Chain distribution as the <strong>prior</strong> distribution (for training our agents offline) and the Generalised Chain distribution as the <strong>test</strong> distribution (for defining the experiment to use to compare the agents).</p>
<h2>
<a aria-hidden="true" class="anchor" href="#step-0---create-an-mdp-distribution" id="user-content-step-0---create-an-mdp-distribution"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Step 0 - Create an MDP distribution</h2>
<p>We create the <strong>Uniform Generalised Chain</strong> distribution<sup><a href="#footnotes">9</a></sup> and the <strong>Generalised Chain</strong> distribution<sup><a href="#footnotes">10</a></sup>.</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --mdp_distrib_generation \
               --name <span class="pl-s"><span class="pl-pds">"</span>Uniform GChain distribution<span class="pl-pds">"</span></span> \
               --short_name <span class="pl-s"><span class="pl-pds">"</span>UGC<span class="pl-pds">"</span></span> \
               --n_states 5 --n_actions 3 \
               --ini_state 0 \
               --transition_weights \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
                    1 1 1 1 1 \
               --reward_type <span class="pl-s"><span class="pl-pds">"</span>RT_CONSTANT<span class="pl-pds">"</span></span> \
               --reward_means \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
               --output <span class="pl-s"><span class="pl-pds">"</span>data/distributions/nUGC-distrib.dat<span class="pl-pds">"</span></span></pre></div>
<p>Note: this distribution is already provided by BBRL in <code>data/distributions/UGC-distrib.dat</code>.</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --mdp_distrib_generation \
               --name <span class="pl-s"><span class="pl-pds">"</span>GChain distribution<span class="pl-pds">"</span></span> \
               --short_name <span class="pl-s"><span class="pl-pds">"</span>GC<span class="pl-pds">"</span></span> \
               --n_states 5 --n_actions 3 \
               --ini_state 0 \
               --transition_weights \
                    1 1 0 0 0 \
                    1 0 1 0 0 \
                    1 0 0 1 0 \
                    1 0 0 0 1 \
                    1 0 0 0 1 \
                    1 1 0 0 0 \
                    1 0 1 0 0 \
                    1 0 0 1 0 \
                    1 0 0 0 1 \
                    1 0 0 0 1 \
                    1 1 0 0 0 \
                    1 0 1 0 0 \
                    1 0 0 1 0 \
                    1 0 0 0 1 \
                    1 0 0 0 1 \
               --reward_type <span class="pl-s"><span class="pl-pds">"</span>RT_CONSTANT<span class="pl-pds">"</span></span> \
               --reward_means \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
                    2 0 0 0 10 \
               --output <span class="pl-s"><span class="pl-pds">"</span>data/distributions/nGC-distrib.dat<span class="pl-pds">"</span></span></pre></div>
<p>Note: this distribution is already provided by BBRL in <code>data/distributions/GC-distrib.dat</code>.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#step-1---offline-learning" id="user-content-step-1---offline-learning"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Step 1 - Offline learning</h2>
<p>We will use two types of agents:</p>
<ul>
<li>An <code>e</code>-Greedy agent: an agent taking random decisions with a probability of <code>e</code>, and "best" decisions else, with respect to a model updated whenever a transition is observed. The offline learning phase is used to compute an initial model as the mean model of the MDP distribution used to train it.</li>
<li>A random agent: an agent taking random decisions.</li>
</ul>
<p>The <code>e</code>-Greedy agents will be instantiated with <code>e = 0.1, 0.25, 0.5</code>.</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span> Random agent</span>
./BBRL-DDS --offline_learning \
                   --agent RandomAgent \
                   --mdp_distribution <span class="pl-s"><span class="pl-pds">"</span>DirMultiDistribution<span class="pl-pds">"</span></span> \
                        --mdp_distribution_file <span class="pl-s"><span class="pl-pds">"</span>data/distributions/nUGC-distrib.dat<span class="pl-pds">"</span></span> \
                   --output <span class="pl-s"><span class="pl-pds">"</span>data/agents/random_agent.dat<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> e-Greedy agent (e = 0.1)</span>
./BBRL-DDS --offline_learning \
                   --agent EGreedyAgent --epsilon 0.1 \
                   --mdp_distribution <span class="pl-s"><span class="pl-pds">"</span>DirMultiDistribution<span class="pl-pds">"</span></span> \
                        --mdp_distribution_file <span class="pl-s"><span class="pl-pds">"</span>data/distributions/nUGC-distrib.dat<span class="pl-pds">"</span></span> \
                   --output <span class="pl-s"><span class="pl-pds">"</span>data/agents/egreedy_agent(0.1).dat<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> e-Greedy agent (e = 0.25)</span>
./BBRL-DDS --offline_learning \
                   --agent EGreedyAgent --epsilon 0.25 \
                   --mdp_distribution <span class="pl-s"><span class="pl-pds">"</span>DirMultiDistribution<span class="pl-pds">"</span></span> \
                        --mdp_distribution_file <span class="pl-s"><span class="pl-pds">"</span>data/distributions/nUGC-distrib.dat<span class="pl-pds">"</span></span> \
                   --output <span class="pl-s"><span class="pl-pds">"</span>data/agents/egreedy_agent(0.25).dat<span class="pl-pds">"</span></span>

<span class="pl-c"><span class="pl-c">#</span> e-Greedy agent (e = 0.5)</span>
./BBRL-DDS --offline_learning \
                   --agent EGreedyAgent --epsilon 0.5 \
                   --mdp_distribution <span class="pl-s"><span class="pl-pds">"</span>DirMultiDistribution<span class="pl-pds">"</span></span> \
                        --mdp_distribution_file <span class="pl-s"><span class="pl-pds">"</span>data/distributions/nUGC-distrib.dat<span class="pl-pds">"</span></span> \
                   --output <span class="pl-s"><span class="pl-pds">"</span>data/agents/egreedy_agent(0.5).dat<span class="pl-pds">"</span></span></pre></div>
<h2>
<a aria-hidden="true" class="anchor" href="#step-2---create-an-experiment" id="user-content-step-2---create-an-experiment"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Step 2 - Create an experiment</h2>
<p>Now, let us create an experiment with:</p>
<ul>
<li>
<strong>50</strong> MDPs drawn from <code>data/distributions/nGC-distrib.dat</code>.</li>
<li>
<strong>1</strong> simulation per MDP.</li>
<li>A discount factor of <strong>0.95</strong>.</li>
<li>A horizon limit of <strong>250</strong>.</li>
</ul>
<div class="highlight highlight-source-shell"><pre>./BBRL-DDS --new_experiment \
                   --name <span class="pl-s"><span class="pl-pds">"</span>GC Experiment<span class="pl-pds">"</span></span> \
                   --mdp_distribution <span class="pl-s"><span class="pl-pds">"</span>DirMultiDistribution<span class="pl-pds">"</span></span> \
                        --mdp_distribution_file <span class="pl-s"><span class="pl-pds">"</span>data/distributions/nGC-distrib.dat<span class="pl-pds">"</span></span> \
                   --n_mdps 50 --n_simulations_per_mdp 1 \
                   --discount_factor 0.95 --horizon_limit 250 \
                   --compress_output \
                   --output <span class="pl-s"><span class="pl-pds">"</span>data/experiments/nGC50-exp.dat<span class="pl-pds">"</span></span></pre></div>
<p>You can find more experiments in <code>data/experiments/</code>.</p>
<h2>
<a aria-hidden="true" class="anchor" href="#step-3---run-an-experiment" id="user-content-step-3---run-an-experiment"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Step 3 - Run an experiment</h2>
<p>Run the experiment for each agent.</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c"><span class="pl-c">#</span> Random agent</span>
./BBRL-DDS --run_experiment \
                   --experiment \
                        --experiment_file <span class="pl-s"><span class="pl-pds">"</span>data/experiments/nGC50-exp.dat.zz<span class="pl-pds">"</span></span> \
                   --agent RandomAgent \
                        --agent_file <span class="pl-s"><span class="pl-pds">"</span>data/agents/random_agent.dat<span class="pl-pds">"</span></span> \
                   --n_threads 1 --compress_output \
                   --output <span class="pl-s"><span class="pl-pds">"</span>data/results/nGC50-random_agent.dat<span class="pl-pds">"</span></span> \
                   --refresh_frequency 1 --backup_frequency 15

<span class="pl-c"><span class="pl-c">#</span> e-Greedy agent (e = 0.1)</span>
./BBRL-DDS --run_experiment \
                   --experiment \
                        --experiment_file <span class="pl-s"><span class="pl-pds">"</span>data/experiments/nGC50-exp.dat.zz<span class="pl-pds">"</span></span> \
                   --agent EGreedyAgent \
                        --agent_file <span class="pl-s"><span class="pl-pds">"</span>data/agents/egreedy_agent(0.1).dat<span class="pl-pds">"</span></span> \
                   --n_threads 1 --compress_output \
                   --output <span class="pl-s"><span class="pl-pds">"</span>data/results/nGC50-egreedy_agent(0.1).dat<span class="pl-pds">"</span></span> \
                   --refresh_frequency 1 --backup_frequency 15

<span class="pl-c"><span class="pl-c">#</span> e-Greedy agent (e = 0.25)</span>
./BBRL-DDS --run_experiment \
                   --experiment \
                        --experiment_file <span class="pl-s"><span class="pl-pds">"</span>data/experiments/nGC50-exp.dat.zz<span class="pl-pds">"</span></span> \
                   --agent EGreedyAgent \
                        --agent_file <span class="pl-s"><span class="pl-pds">"</span>data/agents/egreedy_agent(0.25).dat<span class="pl-pds">"</span></span> \
                   --n_threads 1 --compress_output \
                   --output <span class="pl-s"><span class="pl-pds">"</span>data/results/nGC50-egreedy_agent(0.25).dat<span class="pl-pds">"</span></span> \
                   --refresh_frequency 1 --backup_frequency 15

<span class="pl-c"><span class="pl-c">#</span> e-Greedy agent (e = 0.5)</span>
./BBRL-DDS --run_experiment \
                   --experiment \
                        --experiment_file <span class="pl-s"><span class="pl-pds">"</span>data/experiments/nGC50-exp.dat.zz<span class="pl-pds">"</span></span> \
                   --agent EGreedyAgent \
                        --agent_file <span class="pl-s"><span class="pl-pds">"</span>data/agents/egreedy_agent(0.5).dat<span class="pl-pds">"</span></span> \
                   --n_threads 1 --compress_output \
                   --output <span class="pl-s"><span class="pl-pds">"</span>data/results/nGC50-egreedy_agent(0.5).dat<span class="pl-pds">"</span></span> \
                   --refresh_frequency 1 --backup_frequency 15</pre></div>
<h2>
<a aria-hidden="true" class="anchor" href="#step-4---export-your-results" id="user-content-step-4---export-your-results"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>Step 4 - Export your results</h2>
<p>Finally, we export the results in a Latex report.</p>
<div class="highlight highlight-source-shell"><pre>./BBRL-export --agent RandomAgent \
                      --agent_file <span class="pl-s"><span class="pl-pds">"</span>data/agents/random_agent.dat<span class="pl-pds">"</span></span> \
                 --experiment \
                      --experiment_file <span class="pl-s"><span class="pl-pds">"</span>data/results/nGC50-random_agent.dat.zz<span class="pl-pds">"</span></span> \
                 --agent EGreedyAgent \
                      --agent_file <span class="pl-s"><span class="pl-pds">"</span>data/agents/egreedy_agent(0.1).dat<span class="pl-pds">"</span></span> \
                 --experiment \
                      --experiment_file <span class="pl-s"><span class="pl-pds">"</span>data/results/nGC50-egreedy_agent(0.1).dat.zz<span class="pl-pds">"</span></span> \
                 --agent EGreedyAgent \
                      --agent_file <span class="pl-s"><span class="pl-pds">"</span>data/agents/egreedy_agent(0.25).dat<span class="pl-pds">"</span></span> \
                 --experiment \
                      --experiment_file <span class="pl-s"><span class="pl-pds">"</span>data/results/nGC50-egreedy_agent(0.25).dat.zz<span class="pl-pds">"</span></span> \
                 --agent EGreedyAgent \
                      --agent_file <span class="pl-s"><span class="pl-pds">"</span>data/agents/egreedy_agent(0.5).dat<span class="pl-pds">"</span></span> \
                 --experiment \
                      --experiment_file <span class="pl-s"><span class="pl-pds">"</span>data/results/nGC50-egreedy_agent(0.5).dat.zz<span class="pl-pds">"</span></span> \</pre></div>
<h1>
<a aria-hidden="true" class="anchor" href="#4-i-need-to-write-thousands-of-commands-how-do-i-do-that" id="user-content-4-i-need-to-write-thousands-of-commands-how-do-i-do-that"><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a>4. I need to write thousands of commands! How do I do that?</h1>
<p>In practice, BBRLâ€™s first goal is to provide a nice analysis of different algorithms, helping you to decide which algorithm is the best to consider for a given experiment with respect to its performance and computation time. However, in case you want to test different values for each parameter, the number of commands you need to write and execute can increase very quickly.</p>
<p>To this end, we wrote some scripts in <a href="http://www.lua.org/start.html" rel="nofollow">Lua</a> which do it for you by completing some configuration files in <code>scripts/lua_scripts/data/</code>:</p>
<ul>
<li>
<code>agents.lua</code>: Define your agents, their parameters, and the values to be tested.</li>
<li>
<code>experiments_accurate.lua</code>: Define the experiments you want to conduct (prior distribution = test distribution).</li>
<li>
<code>experiments_inaccurate.lua</code>: Define the experiments you want to conduct (prior distribution != test distribution).</li>
<li>
<code>experiments_options.lua</code>: Define the general options of the experiments (backup frequency, ...).</li>
<li>
<code>formula_sets.lua</code>: Define the formula sets you want to create (OPPS agents).</li>
<li>
<code>gridComputing.lua</code>: Define the settings of your SLURM machine(s)<sup><a href="#footnotes">11</a></sup>.</li>
</ul>
<p>When each file has been completed correctly, you can launch the generation of the scripts as follows:</p>
<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> BBRL/
./scripts/make_scripts.sh [<span class="pl-k">&lt;</span>SLURM name<span class="pl-k">&gt;</span>]</pre></div>
<p>You will get four different scripts in <code>scripts/</code>:</p>
<ul>
<li>
<code>0-init.sh</code>: Create the experiment files, and create the formulas sets required by the OPPS agents.</li>
<li>
<code>1-ol.sh</code>: Create the agents and train them on the prior distribution(s).</li>
<li>
<code>2-re.sh</code>: Run all the experiments.</li>
<li>
<code>3-export.sh</code>: Generate the LATEX reports.</li>
</ul>
<p>Simply run the four scripts to run your experiments:</p>
<div class="highlight highlight-source-shell"><pre>./scripts/0-init.sh
./scripts/1-ol.sh
./scripts/2-re.sh
./scripts/3-export.sh</pre></div>
<p>For SLURM users, the scripts <code>1-ol.sh</code> and <code>2-re.sh</code> will generate many scripts in <code>scripts/</code> (one per job), and submit them directly. If a job is still running, or has already been done, the job will not be submitted again.</p>
<h1>
<a aria-hidden="true" class="anchor" href="#" id=""><svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z" fill-rule="evenodd"></path></svg></a><a name="user-content-footnotes"></a>
</h1>
<p><sub>1: An agent which makes random decisions.</sub><br/>
<sub>2: An agent which knows the transition function used by the actual MDP. If you want to test this agent, your experiment must be created without the <code>--safe_simulations</code> flag.</sub><br/>
<sub>3: An agent which makes random decisions with a probability of <code>e</code>, and follows the policy of another agent else.</sub><br/>
<sub>4: An agent which makes random decisions with a probability of <code>e</code>, and greedy decisions with respect to a model else.</sub><br/>
<sub>5: An agent which makes its decisions randomly, where the probability to draw an action is proportional to its estimated quality. The quality of an action is estimated by a model.</sub><br/>
<sub>6: This agent does not support multi-threading. It has been developed by A. Guez. Feel free to visit his <a href="http://www.cs.mcgill.ca/%7Eaguez/" rel="nofollow">website</a> and <a href="https://github.com/acguez/bamcp">repository</a> for more information.</sub><br/>
<sub>7: This is an experimental algorithm, which generalises the approach used in <a href="http://orbi.ulg.ac.be/handle/2268/127985" rel="nofollow">OPPS-DS</a> to continuous sets of strategies. We first define a set of continuous strategies which is parameterised by some values. Then, we define a function whose inputs are the parameters of a strategy, and whose output is the discounted sum of the returns observed when testing this strategy on an MDP drawn from the prior MDP distribution. Eventually, we use a function optimisation algorithm to optimise this function. Since the evaluations of this function are noisy (because the MDP is drawn randomly and the strategy is stochastic), we chose to use the <a href="https://sequel.lille.inria.fr/Software/StoSOO" rel="nofollow">Sto-SOO algorithm</a>.</sub><br/>
<sub>8: An agent which trains an Artificial Neural Networks for taking decisions in MDPs.</sub><br/>
<sub>9: This will be our <strong>prior</strong> distribution.</sub><br/>
<sub>10: This will be our <strong>test</strong> distribution.</sub><br/>
<sub>11: SLURM is a system which is suited to run many jobs through a network of different machines. You should ensure that each cluster provides the same amount of power in order to have relevant results. See <a href="http://slurm.schedmd.com" rel="nofollow">here</a> for more information.</sub></p>

        </div>

    </div>]